{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tVYA302cLyfP"
   },
   "source": [
    "# Provable Robustness for Deep Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3LbEBZGKLyfZ"
   },
   "source": [
    "In this notebook, we will implement the robustness certificate that we derived in the PDF. That is, we will first define and train a three-layer neural classifier; then, we will calculate its dual, and using this, check whether the classifier is dual at given input points.\n",
    "\n",
    "**Your task is to fill in any sections labeled TODO in the code and answer the bolded questions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GJMgQYklLyfd"
   },
   "source": [
    "We are using torch here; for our purposes, we can think of torch as essentially numpy with GPU support and and automatic differentiation. That is, for any function we compute, torch automatically keeps track of the function's gradient with respect to inputs; this will make gradient descent much easier to implement. \n",
    "\n",
    "The primary object you will need to manipulate here is `torch.Tensor`, which can be thought of as equivalent to  `np.array`. Indexing, splicing, multiplication, etc. will work like you would expect them to in numpy.\n",
    "\n",
    "Also, most of the numpy functions you are used to are present in torch, with the same name. E.g:\n",
    "* `np.max(input, axis)` --> `torch.max(input, dim)` (Note that `torch.max` actually returns a tuple of the max and argmax).\n",
    "* `np.zeros` --> `torch.zeros`\n",
    "* `np.eye` --> `torch.eye`\n",
    "* `np.linalg.norm(x, ord, axis)` --> `torch.norm(input, p, dim)`\n",
    "\n",
    "For more information, refer to the [torch documentation](https://pytorch.org/docs/stable/torch.html) or the [torch tutorials](https://pytorch.org/tutorials/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L3ChNfnaLBbw"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PczgocJtLBb0"
   },
   "source": [
    "Here, we import the relevant libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y3oepvCSLyfi"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DPbnELbsLBcM"
   },
   "source": [
    "This line tells torch to use the GPU if available, and otherwise the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "hBpT1WebQ394",
    "outputId": "4316775c-297a-4541-c43c-89efc5bcb9c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "awgBbuiRLyf0"
   },
   "source": [
    "Here, we load in the MNIST dataset. The inputs are $28\\times 28$ images of handwritten digits, while the labels are the corresponding digit. Note that we split the data between a training set and a test set. In order to have an unbiased estimate of the classifier's performance, we must train the model only on the training set (**never the test set**), then test its accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334,
     "referenced_widgets": [
      "c33f5dd2c1f443f1b771f0c46054eebf",
      "9601fc4be35543a784d4231a2de31edb",
      "8c6d1f7d352a4c089e4ae601a73c1068",
      "3ce16927200e4dfbba645b52ede3be15",
      "8a08a24593a2422985302484dc4594d1",
      "410b4797b4eb474185b6e9e8768efe90",
      "7f3be4825b3942bfbd15404ae2826968",
      "51fab2724f5240f2a197da36bfb5d37f",
      "1e0fc65863e247bba684aa570176e47e",
      "0ffb65b8fcd64017a1575ef5c6fb6783",
      "9b0d7cc1d4a84b32ad704d0f92aaf7c2",
      "359ca0eaed76461b8920085a62f10151",
      "0d2ab5e4e4634c2d84445dcb59e63093",
      "c75be203b59b4c599fd51b44b5359236",
      "7cb630c6d3784c0ba705b39311a36690",
      "080f0e9e20374c8b9d0772c207d40204",
      "443bc19134b346e282b58b81de72ee9b",
      "be2ab404919947b6bec6e47d8ef2293e",
      "d51c1aaaee7e40eb9a6946ee964f26db",
      "75a9df895ea94bb787ead957f9b540c9",
      "40781e910ad14961b70299654d08af57",
      "48f34dff73514ae58be4a78b897605de",
      "73155d071971464cac0ed23138cde635",
      "ad74039e867d489fbcc6c9d8ff34706c",
      "bde4879694234dc087cc6ef3a8f999e4",
      "86cdc4424b24473a8d571d2160ee7576",
      "9d5649b6391e44f5ad266c65fe71ec8e",
      "2dfa9272bd3a454582ebcd023d814301",
      "68fb67ba49274e2381586dfa12d4a00a",
      "de997021a80b4943b20c5518910ea387",
      "77a3b32f508c46f4a5bae2a29bbfc4f2",
      "5cdd60a9989c4e2fba367aa0fc23db04"
     ]
    },
    "colab_type": "code",
    "id": "q58IyfpGLyf4",
    "outputId": "73cb8853-c645-4bf7-d8e5-392613148c4a"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n4iXn51ILygD"
   },
   "source": [
    "This is a utility function to visualize torch Tensors as images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3EkoFimALygG"
   },
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    '''\n",
    "    Visualizes IMG.\n",
    "    IMG should be a 2D torch Tensor.\n",
    "    '''\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.detach().numpy()\n",
    "    plt.imshow(npimg, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Iu56FwY-LBc9"
   },
   "source": [
    "## Primal Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zAc2QEcALygQ"
   },
   "source": [
    "Here, we define the neural classifier we will be using. Note that the network comprises three layers. The first layer has dimension $28^2$ since this is the size of the input image. (The original inputs are square images, but we flatten them into a $28^2\\times 1$ vector in order to feed them into the network.) The output layer has dimension $10$, since there are ten possible output classes (the digits 0-9). The hidden layer has dimension $256$. (There isn't as much science behind choosing the dimensionality of input layers; we choose $256$ because it is a round number, and is hopefully enough to the neccesary encode information about the input image.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "6-joaa2DLygU",
    "outputId": "512b7136-2b3a-4487-837d-f8b3f554f582"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.fc1 = nn.Linear(in_features=28*28, out_features = 256)\n",
    "    self.fc2 = nn.Linear(in_features=256, out_features = 10)\n",
    "    self.layers = [self.fc1, self.fc2]\n",
    "\n",
    "  # define forward function\n",
    "  def forward(self, t):\n",
    "    '''\n",
    "    On input T, performs a affine transformation, then\n",
    "    a ReLU, then another affine transformation.\n",
    "    '''\n",
    "    self.z = []\n",
    "    t = t.reshape(-1, 28*28)\n",
    "    t = self.fc1(t)\n",
    "    self.z.append(t)\n",
    "    t = F.relu(t)\n",
    "    t = self.fc2(t)\n",
    "    self.z.append(t)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YTue7LMGLygd"
   },
   "source": [
    "Here is the training code, which uses Adam, a variant of gradient descent. The actual optimization machinery is all abstracted away behind the torch library; all the work is being done by the `optimizer.step()` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4zdMiOXDLygf"
   },
   "outputs": [],
   "source": [
    "def train(net, criterion, trainloader, lr=0.001):\n",
    "    '''\n",
    "    Uses the Adam optimization algorithm to train \n",
    "    the classifier NET on training data from TRAINLOADER,\n",
    "    on loss function CRITERION, with learning rate LR.\n",
    "    \n",
    "    Note that we half the learning rate each epoch.\n",
    "    '''\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(3):\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr * 0.5 ** (epoch)\n",
    "\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            if i % 500 == 0:\n",
    "                print('Epoch', epoch, 'Iter:', i, 'Loss', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zmx06HSLLyhh"
   },
   "source": [
    "We can now train the net on the training data, using cross entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "colab_type": "code",
    "id": "W8pmvL6tLyhj",
    "outputId": "4d4e82cf-a525-485b-914c-d1afae757338"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Iter: 0 Loss 2.397629499435425\n",
      "Epoch 0 Iter: 500 Loss 0.09621503949165344\n",
      "Epoch 0 Iter: 1000 Loss 2.021833896636963\n",
      "Epoch 0 Iter: 1500 Loss 3.179950475692749\n",
      "Epoch 0 Iter: 2000 Loss 0.7570248246192932\n",
      "Epoch 0 Iter: 2500 Loss 0.18895959854125977\n",
      "Epoch 0 Iter: 3000 Loss 0.039231717586517334\n",
      "Epoch 0 Iter: 3500 Loss 1.316033124923706\n",
      "Epoch 0 Iter: 4000 Loss 0.30988988280296326\n",
      "Epoch 0 Iter: 4500 Loss 0.2024785429239273\n",
      "Epoch 0 Iter: 5000 Loss 0.01707061380147934\n",
      "Epoch 0 Iter: 5500 Loss 0.5606357455253601\n",
      "Epoch 0 Iter: 6000 Loss 0.1921745091676712\n",
      "Epoch 0 Iter: 6500 Loss 0.025904109701514244\n",
      "Epoch 0 Iter: 7000 Loss 0.8297107815742493\n",
      "Epoch 0 Iter: 7500 Loss 0.022795086726546288\n",
      "Epoch 0 Iter: 8000 Loss 0.01586798205971718\n",
      "Epoch 0 Iter: 8500 Loss 0.08148607611656189\n",
      "Epoch 0 Iter: 9000 Loss 0.09096473455429077\n",
      "Epoch 0 Iter: 9500 Loss 0.03333099186420441\n",
      "Epoch 0 Iter: 10000 Loss 1.0368075370788574\n",
      "Epoch 0 Iter: 10500 Loss 0.1712825447320938\n",
      "Epoch 0 Iter: 11000 Loss 0.05824394151568413\n",
      "Epoch 0 Iter: 11500 Loss 0.04343051835894585\n",
      "Epoch 0 Iter: 12000 Loss 0.021558335050940514\n",
      "Epoch 0 Iter: 12500 Loss 0.008385160006582737\n",
      "Epoch 0 Iter: 13000 Loss 0.18716368079185486\n",
      "Epoch 0 Iter: 13500 Loss 0.0965704545378685\n",
      "Epoch 0 Iter: 14000 Loss 0.1713501513004303\n",
      "Epoch 0 Iter: 14500 Loss 0.1560748964548111\n",
      "Epoch 1 Iter: 0 Loss 0.045845143496990204\n",
      "Epoch 1 Iter: 500 Loss 0.09152962267398834\n",
      "Epoch 1 Iter: 1000 Loss 0.01492752879858017\n",
      "Epoch 1 Iter: 1500 Loss 0.007879332639276981\n",
      "Epoch 1 Iter: 2000 Loss 0.03674119710922241\n",
      "Epoch 1 Iter: 2500 Loss 0.000515251886099577\n",
      "Epoch 1 Iter: 3000 Loss 0.034294743090867996\n",
      "Epoch 1 Iter: 3500 Loss 0.05038469657301903\n",
      "Epoch 1 Iter: 4000 Loss 0.10410279035568237\n",
      "Epoch 1 Iter: 4500 Loss 0.0011476593790575862\n",
      "Epoch 1 Iter: 5000 Loss 0.0015435179229825735\n",
      "Epoch 1 Iter: 5500 Loss 0.002257675863802433\n",
      "Epoch 1 Iter: 6000 Loss 0.02678084373474121\n",
      "Epoch 1 Iter: 6500 Loss 0.004342169500887394\n",
      "Epoch 1 Iter: 7000 Loss 0.001047875382937491\n",
      "Epoch 1 Iter: 7500 Loss 0.0005352968582883477\n",
      "Epoch 1 Iter: 8000 Loss 0.0019193228799849749\n",
      "Epoch 1 Iter: 8500 Loss 0.07949910312891006\n",
      "Epoch 1 Iter: 9000 Loss 0.0005158965941518545\n",
      "Epoch 1 Iter: 9500 Loss 0.0027995766140520573\n",
      "Epoch 1 Iter: 10000 Loss 0.019988711923360825\n",
      "Epoch 1 Iter: 10500 Loss 0.0014192508533596992\n",
      "Epoch 1 Iter: 11000 Loss 0.0054008872248232365\n",
      "Epoch 1 Iter: 11500 Loss 0.2154463678598404\n",
      "Epoch 1 Iter: 12000 Loss 0.02350170724093914\n",
      "Epoch 1 Iter: 12500 Loss 0.0032653952948749065\n",
      "Epoch 1 Iter: 13000 Loss 1.1927428245544434\n",
      "Epoch 1 Iter: 13500 Loss 0.00080283940769732\n",
      "Epoch 1 Iter: 14000 Loss 0.04232773929834366\n",
      "Epoch 1 Iter: 14500 Loss 0.027176057919859886\n",
      "Epoch 2 Iter: 0 Loss 0.7375697493553162\n",
      "Epoch 2 Iter: 500 Loss 0.0007070326246321201\n",
      "Epoch 2 Iter: 1000 Loss 0.050150152295827866\n",
      "Epoch 2 Iter: 1500 Loss 0.0008819744689390063\n",
      "Epoch 2 Iter: 2000 Loss 0.000407427956815809\n",
      "Epoch 2 Iter: 2500 Loss 0.012674732133746147\n",
      "Epoch 2 Iter: 3000 Loss 0.0015544299967586994\n",
      "Epoch 2 Iter: 3500 Loss 0.25028014183044434\n",
      "Epoch 2 Iter: 4000 Loss 0.030254555866122246\n",
      "Epoch 2 Iter: 4500 Loss 0.08745969831943512\n",
      "Epoch 2 Iter: 5000 Loss 0.020198408514261246\n",
      "Epoch 2 Iter: 5500 Loss 1.1503590030770283e-05\n",
      "Epoch 2 Iter: 6000 Loss 0.005631211679428816\n",
      "Epoch 2 Iter: 6500 Loss 1.0932601690292358\n",
      "Epoch 2 Iter: 7000 Loss 0.2633860111236572\n",
      "Epoch 2 Iter: 7500 Loss 0.2293035089969635\n",
      "Epoch 2 Iter: 8000 Loss 0.052151553332805634\n",
      "Epoch 2 Iter: 8500 Loss 0.950538158416748\n",
      "Epoch 2 Iter: 9000 Loss 0.2784242331981659\n",
      "Epoch 2 Iter: 9500 Loss 0.004357874859124422\n",
      "Epoch 2 Iter: 10000 Loss 0.0004768501967191696\n",
      "Epoch 2 Iter: 10500 Loss 0.0003871480585075915\n",
      "Epoch 2 Iter: 11000 Loss 0.00685888109728694\n",
      "Epoch 2 Iter: 11500 Loss 0.014271422289311886\n",
      "Epoch 2 Iter: 12000 Loss 0.003905730787664652\n",
      "Epoch 2 Iter: 12500 Loss 0.0028975701425224543\n",
      "Epoch 2 Iter: 13000 Loss 0.002636814257130027\n",
      "Epoch 2 Iter: 13500 Loss 0.00011350725253578275\n",
      "Epoch 2 Iter: 14000 Loss 0.004791347309947014\n",
      "Epoch 2 Iter: 14500 Loss 0.012382342480123043\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train(net, criterion, trainloader, 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "12zEbgnwLBdg"
   },
   "source": [
    "Let's load a sample image from the test dataset, and see what the classifier makes of it. Make sure to visualize the image using `imshow(x[0,0])`. Also, note that the line `test_iter.next()` pulls a new input image from the test set each time you run it; try running the next code block a few times to get a sense of what the MNIST dataset looks like, and how the classifier performs on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NFBwByD1LBdi"
   },
   "outputs": [],
   "source": [
    "test_iter = iter(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "colab_type": "code",
    "id": "sB_m2TzpONdu",
    "outputId": "f09820e2-65df-4c6e-80fe-32432bd510b4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier output: tensor([[ -5.7322, -11.1446,  -1.2278,  -1.1003, -16.0711,  -7.1683, -24.0742,\n",
      "           7.2680,  -5.4176,  -1.8088]])\n",
      "Classifier prediction: 7\n"
     ]
    }
   ],
   "source": [
    "x, labels = test_iter.next()\n",
    "x = x[0].unsqueeze(0)\n",
    "labels = labels[0].unsqueeze(0)\n",
    "imshow(x[0,0])\n",
    "\n",
    "x = x.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "out = net(x).data\n",
    "print('Classifier output:', out)\n",
    "print('Classifier prediction:', torch.argmax(out).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b6nekayzLygn"
   },
   "source": [
    "We can also measure the classifier's accuracy on the full test dataset. This function takes in a classifier we have trained and the loader for the test set, and outputs the classifier's accuracy. The accuracy is simply\n",
    "$$ \\dfrac{\\text{# correct}}{\\text{# total}}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lk4pq6hiLygr"
   },
   "outputs": [],
   "source": [
    "def accuracy(net, testloader):\n",
    "    '''\n",
    "    Returns the accuracy of classifier NET\n",
    "    on test data from TESTLOADER.\n",
    "    '''\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "kXpxjHkqLBd8",
    "outputId": "96b7766c-7368-4df9-dc0e-bc1c55b03d67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy on original test dataset: 0.9692\n"
     ]
    }
   ],
   "source": [
    "print('Classifier accuracy on original test dataset:', accuracy(net, testloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rrvBEzmZLBeD"
   },
   "source": [
    "## Fast Gradient Sign Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p5ZlWjqILygz"
   },
   "source": [
    "Here, we implement the Fast Gradient Sign Method, which takes in a batch of input images, their labels, a trained classifier, and the epsilon radius within which the perturbation should lie. This function should output the input image perturbed in the direction of the sign of the gradient with respect to the classifier's loss.\n",
    "\n",
    "(Note that the output is not guaranteed to lie in the valid range for images, since here pixel values must be in $[-1,1]$. You should use `torch.clamp` to fix the FGSM output to lie in the correct range.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CaSK1tjSLyg2"
   },
   "outputs": [],
   "source": [
    "def FGSM(x, labels, net, eps):\n",
    "    '''\n",
    "    Given an input image X and its corresponding labels\n",
    "    LABELS, as well as a classifier NET, returns X\n",
    "    perturbed by EPS using the fast gradient sign method.\n",
    "    '''\n",
    "    net.zero_grad()    # Zero out any gradients from before\n",
    "    x.requires_grad=True    # Keep track of gradients\n",
    "    out = net(x)    # Output of classifier\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss = criterion(out, labels)   # Classifier's loss\n",
    "    loss.backward()\n",
    "    grads = x.grad.data    # Gradient of loss w/r/t input\n",
    "    return x + eps * grads.sign()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AB8K0vMsLBeU"
   },
   "source": [
    "Let's see how well the classifier does when the input is adversarially perturbed using FGSM. Try this for $\\varepsilon\\in\\{0.05, 0.1,0.2,0.3, 0.4\\}$, and again remember to visualize the inputs with `imshow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.25 # TODO: Try eps = 0.05, 0.1, 0.2, 0.3, 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "colab_type": "code",
    "id": "dbS2KvG6Lyh6",
    "outputId": "da1fe90b-4789-416b-f26c-2e950e9c33ad"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier output: tensor([[ -1.2083,  -7.8354,   3.6865,   2.4477, -19.7304,  -2.3483, -15.9539,\n",
      "          -1.3925,   0.4301,  -3.8607]])\n",
      "Classifier prediction: 2\n"
     ]
    }
   ],
   "source": [
    "# We are using the same sample input x as before.\n",
    "x.requires_grad = True\n",
    "x_prime = FGSM(x, labels, net, eps)\n",
    "imshow(x_prime[0,0].cpu())\n",
    "out = net(x_prime)\n",
    "\n",
    "print('Classifier output:', out.data)\n",
    "print('Classifier prediction:', torch.argmax(out).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "64qakoWaLBec"
   },
   "source": [
    "We should evaluate the classifier's performance on FGSM-perturbed data by the same metric that we will later use in the primal adversarial problem. That is, for the classifier's output vector $\\vec{\\hat{z}}_3$, we want to compute\n",
    "$$\n",
    "\\vec{c}_j^\\top \\vec{\\hat{z}}_3\n",
    "$$\n",
    "where\n",
    "$$\\vec{c}_j={\\vec{y}_{\\text{true}}}-\\vec{e}_{j}$$\n",
    "for each $j\\in[10]$.\n",
    "\n",
    "Recall that \n",
    "$$\\vec{c}_j^\\top \\vec{\\hat{z}}_3=\\vec{\\hat{z}}_{3i_{\\text{true}}}-\\vec{\\hat{z}}_{3j},$$\n",
    "i.e. $\\vec{c}_j^\\top \\vec{\\hat{z}}_3$ is the difference between the classifier's confidence on the true class and the $j$th (incorrect) class. If $\\vec{c}_j^\\top \\vec{\\hat{z}}_3$ is positive for all incorrect $j$, then the classifier was not fooled by the adversarial perturbation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "qpFjSe3ULBee",
    "outputId": "6256f017-d100-4f24-b932-ea6c67b8710e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -0.18424642086029053\n",
      "1 6.442866325378418\n",
      "2 -5.079038619995117\n",
      "3 -3.840223550796509\n",
      "4 18.337902069091797\n",
      "5 0.9557883739471436\n",
      "6 14.561408996582031\n",
      "8 -1.8226368427276611\n",
      "9 2.4681620597839355\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    c = torch.zeros(10, 1).to(device)\n",
    "    if i != labels:\n",
    "        c[i] = -1\n",
    "        c[labels] = 1\n",
    "        print(i, (out @ c).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q4JTyFqQLBeo"
   },
   "source": [
    "**Q: What do the $\\vec{c}_j^\\top \\vec{\\hat{z}}_3$ scores tell you about the robustness of the classifier to different values of epsilon? For a given input digit, which output categories have higher/lower scores? Why?**\n",
    "\n",
    "A: Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LaGNmioNLyg_"
   },
   "source": [
    "Now that FGSM is defined, we can also measure a classifier's accuracy on a dataset where each input has been adversarially perturbed. That is, for each point in the original test dataset, we first perturb it using FGSM before feeding it to the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ik9xXp3hLyhB"
   },
   "outputs": [],
   "source": [
    "def accuracy_on_FGSM(net, testloader, eps):\n",
    "    '''\n",
    "    Returns the accuracy of classifier NET on test\n",
    "    data from TESTLOADER that has been perturbed by\n",
    "    EPS using FSGM.\n",
    "    '''\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in testloader:\n",
    "        x, labels = data[0].to(device), data[1].to(device)\n",
    "        x_prime = FGSM(x, labels, net, eps)\n",
    "        outputs = net(x_prime)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "PDD_z76ALBew",
    "outputId": "01ccf72d-1bab-4ce7-ab00-d202779e0937"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy on test dataset perturbed with FGSM: 0.1675\n"
     ]
    }
   ],
   "source": [
    "print('Classifier accuracy on test dataset perturbed with FGSM:', accuracy_on_FGSM(net, testloader, eps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dj-7r-oqLBe1"
   },
   "source": [
    "**Q: How does the classifier accuracy on data perturbed by FGSM compare to that on the original test dataset? How does this vary with epsilon?**\n",
    "\n",
    "A: The classifier accuracy on the data perturbed by FGSM is **signficantly lower** than the test set accuracy. As epsilon increases the FGSM attack becomes stronger and the perturbed data accuracy approaches 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Leave One Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iter = iter(testloader)\n",
    "x, labels = test_iter.next()\n",
    "x_adv = FGSM(x, labels, net, 0.12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow(x_adv[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGD(nn.Module):\n",
    "    def __init__(self, epsilon=8./255, num_steps=10, step_size=2./255, verbose=False):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.num_steps = num_steps\n",
    "        self.step_size = step_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def forward(self, model, bx, by):\n",
    "        \"\"\"\n",
    "        :param model: the classifier's forward method\n",
    "        :param bx: batch of images\n",
    "        :param by: true labels\n",
    "        :return: perturbed batch of images\n",
    "        \"\"\"\n",
    "        adv_bx = bx.detach()\n",
    "        adv_bx += torch.zeros_like(adv_bx).uniform_(-self.epsilon, self.epsilon)\n",
    "        \n",
    "        for i in range(self.num_steps):\n",
    "            adv_bx.requires_grad_()\n",
    "            \n",
    "            with torch.enable_grad():\n",
    "                logits = model(adv_bx)\n",
    "                cent_loss = F.cross_entropy(logits, by, reduction='mean')\n",
    "                loss = cent_loss\n",
    "                if self.verbose:\n",
    "                    print(\"Step: {}, Cent: {}\".format(i, cent_loss))\n",
    "                    \n",
    "            grad = torch.autograd.grad(loss, adv_bx, only_inputs=True)[0]\n",
    "            adv_bx = adv_bx.detach() + self.step_size * torch.sign(grad.detach())\n",
    "            adv_bx = torch.min(torch.max(adv_bx, bx - self.epsilon), bx + self.epsilon)\n",
    "            \n",
    "        return adv_bx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_saliency_map(img, h, w, fx, pred):\n",
    "    saliency_map = np.zeros((h, w))\n",
    "    mask = torch.ones((h, w))\n",
    "    \n",
    "    for i in range(h):\n",
    "        for j in range(w): \n",
    "            mask[i][j] = 0\n",
    "            out = net(img * mask).data\n",
    "            mask[i][j] = 1\n",
    "            saliency_map[i][j] = fx - out[0][pred]\n",
    "                        \n",
    "    return saliency_map\n",
    "\n",
    "def norm(sm):\n",
    "    return (sm - sm.min())/(sm.max() - sm.min())\n",
    "\n",
    "def saliency_maps_for_imgs(attacker, img, label):\n",
    "    h, w = img.shape\n",
    "\n",
    "    out = net(img).data\n",
    "    pred = torch.argmax(out).item()\n",
    "    fx = out[0][pred]\n",
    "    assert label == pred\n",
    "    \n",
    "    adv_img = attacker(net, img, torch.tensor([pred]))\n",
    "    out_adv = net(adv_img).data\n",
    "    pred_adv = torch.argmax(out_adv).item()\n",
    "    fx_adv = out_adv[0][pred_adv]\n",
    "    \n",
    "    assert pred_adv != pred\n",
    "    \n",
    "    sm = norm(calculate_saliency_map(img, h, w, fx, pred))\n",
    "    sm_adv = norm(calculate_saliency_map(adv_img, h, w, fx_adv, pred_adv))\n",
    "    \n",
    "    return sm, sm_adv, adv_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9d91e05340e42b3a749aa309e2c9df1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.6, description='epsilon', max=1.7999999999999998, min=-0.6), IntSlid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def disp_maps(sm, sm_adv, img, adv_img):\n",
    "    row = 1 \n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(14, 14))\n",
    "\n",
    "    axs[row, 0].set_title(\"Regular Saliency Map\")\n",
    "    img0 = axs[row, 0].imshow(sm, cmap=\"hot\")\n",
    "\n",
    "    axs[row, 1].set_title(\"ADV Saliency Map\")\n",
    "    axs[row, 1].imshow(sm_adv, cmap=\"hot\")\n",
    "\n",
    "    axs[1-row, 0].set_title(\"Regular – Saliency Map x Image\")\n",
    "    axs[1-row, 0].imshow(sm * img, cmap=\"hot\")\n",
    "\n",
    "    axs[1-row, 1].set_title(\"Adv – Saliency Map x Image\")\n",
    "    axs[1-row, 1].imshow(sm_adv * adv_img, cmap=\"hot\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "@interact\n",
    "def run_iter(epsilon = 0.6, steps = 10, step_size=0.3):\n",
    "    test_iter = iter(testloader)\n",
    "    test_iter.next()\n",
    "    x, labels = test_iter.next()\n",
    "\n",
    "    attacker = PGD(epsilon=0.60, num_steps=steps, step_size=50/255, verbose=False)\n",
    "    sm, sm_adv, adv_img = saliency_maps_for_imgs(attacker, x[0,0], labels[0])\n",
    "    \n",
    "    disp_maps(sm, sm_adv, norm(x[0,0].detach().numpy()), norm(adv_img.detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jDwEDxf9LBe7"
   },
   "source": [
    "## Dual Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AkVG-qEuLyhK"
   },
   "source": [
    "Here, we will implement the dual network. First, we write the function to compute upper and lower bounds for the dual network. This function should take an input image, the trained classifier, and an epsilon value, and return the tuple\n",
    "$$(\\vec{l},\\vec{u},S,S^-,S^+)$$\n",
    "where $\\vec{u}$ and $\\vec{l}$ are the upper and lower bounds, respectively, for the input to the ReLU layer, and $S^-,S^+,S$ are sets defined by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&S:=\\{j\\in [n_2]\\mid l_{j}\\leq 0\\leq u_{j}\\}\\\\\n",
    "&S^{-}:=\\{j\\in [n_2]\\mid l_{j}\\leq u_{j}\\leq 0\\}\\\\\n",
    "&S^{+}:=\\{j\\in [n_2]\\mid 0\\leq l_{j}\\leq u_{j}\\}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "See Section 6 of the PDF for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_0QHBHTHLyhO"
   },
   "outputs": [],
   "source": [
    "def dual_bounds(x, net, eps):\n",
    "    '''\n",
    "    Given a classifier NET, an input image X,\n",
    "    and the epsilon parameter EPS, returns the lower\n",
    "    and upper bounds L and U respectively, as well as\n",
    "    the corresponding sets S, S_MIN, S_PLUS.\n",
    "    '''\n",
    "    x = x[0].reshape(-1, 1)    # Reshape input to more convenient dimensions\n",
    "    W = [layer.weight for layer in net.layers]    # Array of network weights (W matrices)\n",
    "    b = [layer.bias.reshape(-1, 1) for layer in net.layers]    # Array of network biases (b vectors)\n",
    "    n = W[1].shape[1]    # Dimensionality of hidden layer\n",
    "    u = # TODO\n",
    "    l = # TODO\n",
    "    S = # TODO\n",
    "    S_plus = # TODO\n",
    "    S_min = # TODO\n",
    "    return l, u, S, S_min, S_plus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VaMhcqTvLyhX"
   },
   "source": [
    "Given the tuple $(l,u,S,S^-,S^+)$, we are ready to calculate the dual objective itself. This function should take in an input image, the classifier, a vector $c$, and the $(l,u,S,S^-,S^+)$ from the previous function in order to output \n",
    "$$\n",
    "d^*(\\vec{x},\\vec{c})= \n",
    "-\\vec{\\hat{\\nu}}_1^\\top \\vec{x}-\\varepsilon\\|\\vec{\\hat{\\nu}}_1\\|_1-\\sum_{i=1}^{2}\\vec{\\nu}_{i+1}^\\top \\vec{b}_i+\\sum_{j\\in S\\\n",
    "}l_{j}\\text{ReLU}(\\nu_{2j})\n",
    "$$\n",
    "\n",
    "Where the $\\vec{\\nu}$ vectors are computed as\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\vec{\\nu}_3=-\\vec{c}\\\\\n",
    "&\\vec{\\hat{\\nu}}_2=W_2^\\top \\vec{\\nu}_{3}\\\\\n",
    "&\\nu_{2j}=0 && \\forall j\\in S^-\\\\\n",
    "&\\nu_{2j}=\\hat{\\nu}_{2j} && \\forall j\\in S^+\\\\\n",
    "&\\nu_{2j}=\\dfrac{u_{j}}{u_{j}-l_{j}}\\hat{\\nu}_{2j} && \\forall j\\in S\\\\\n",
    "&\\vec{\\hat{\\nu}}_1=W_1^\\top \\vec{\\nu_{2}}\n",
    "&\\end{aligned}.\n",
    "$$\n",
    "\n",
    "Again, see Section 6 of the PDF for more details.\n",
    "\n",
    "One efficient way to compute $\\vec{\\nu}_2$ is to rewrite it as\n",
    "$$\\vec{\\nu}_2= D\\vec{\\hat{\\nu}}_2,$$\n",
    "where $D$ is a diagonal matrix defined  by\n",
    "$$\n",
    "D_{jj}=\\begin{cases}\n",
    "0 & j\\in S^-\\\\\n",
    "\\hat{\\nu}_{2j} & j\\in S^+\\\\\n",
    "\\dfrac{u_{j}}{u_{j}-l_{j}}\\hat{\\nu}_{2j} & j\\in S.\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "chl5lue0Lyha"
   },
   "outputs": [],
   "source": [
    "# Constructs the diagonal D matrix from the S sets, n (the dimensionality\n",
    "# of the hidden layer), u, and l.\n",
    "def StoD(S_min, S_plus, S, n, u, l):\n",
    "    '''\n",
    "    Given upper and lower bounds U and L, as well\n",
    "    as the corresponding sets S_MIN, S_PLUS, and S,\n",
    "    as well as the dimension of the hidden layer N,\n",
    "    returns the corresponding diagonal matrix D.\n",
    "    '''\n",
    "    d = []\n",
    "    for j in range(n):\n",
    "        if j in S:\n",
    "            d.append((u[j] / (u[j] - l[j])).item())\n",
    "        elif j in S_plus:\n",
    "            d.append(1)\n",
    "        elif j in S_min:\n",
    "            d.append(0)\n",
    "        else:\n",
    "            assert False, 'StoD error.'\n",
    "    return torch.diag(torch.Tensor(d)).to(device)\n",
    "\n",
    "def dual_forward(x, net, c, eps, l, u, S, S_min, S_plus):\n",
    "    '''\n",
    "    Calculates the dual objective for classifier NET with input X\n",
    "    and dual input C and epsilon parameter S. Depends on lower\n",
    "    and upper bounds L and U, as well as the corresponding sets\n",
    "    S, S_MIN, S_PLUS.\n",
    "    '''\n",
    "    x = x[0].reshape(-1, 1)    # Reshape input to more convenient dimensions\n",
    "    W = [layer.weight for layer in net.layers]    # Array of network weights (W matrices)\n",
    "    b = [layer.bias.reshape(-1, 1) for layer in net.layers]    # Array of network biases (b vectors)\n",
    "    n = W[1].shape[1]    # Dimensionality of hidden layer\n",
    "    D = StoD(S_min, S_plus, S, n, u, l)\n",
    "    # TODO: Your code here!\n",
    "    return # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HagySRiNLyhu"
   },
   "source": [
    "Now, we can use the dual network to check the robustness of the network we just trained on sample input images. We can do this for \n",
    "$$\\vec{c}_j={\\vec{y}_{\\text{true}}}-\\vec{e}_{j}$$\n",
    "for each $j\\in[10]$.\n",
    "\n",
    "The output is a vector where the $j$th element is the difference between the model's confidence in the true class and the $j$th class; if $d^*(\\vec{x},\\vec{c}_j)$ is nonnegative for every $j\\in[10]$, then we know the model is robust to perturbations of size $\\varepsilon$. See Section 8 of the PDF for more details.\n",
    "\n",
    "Try running the following block of code for different values of $\\varepsilon\\in\\{0.05, 0.1, 0.2, 0.3, 0.4\\}$, and compare the robustness guarantees here with the classifier's performance on the FGSM data from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.05 # TODO: Try eps = 0.05, 0.1, 0.2, 0.3, 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "WY6TfL2GLyhw",
    "outputId": "41886f00-c9f7-4f6d-b227-1bb5fe647a19"
   },
   "outputs": [],
   "source": [
    "# We are still using the same sample input x as before.\n",
    "l, u, S, S_min, S_plus = dual_bounds(x, net, eps)\n",
    "\n",
    "# Here, we loop through each column c_j defined above, and output the \n",
    "# objective value for the dual function with input c.\n",
    "for i in range(10):\n",
    "    c = torch.zeros(10, 1).to(device)\n",
    "    if i != labels:\n",
    "        c[i] = -1\n",
    "        c[labels] = 1\n",
    "        print(i, dual_forward(x, net, c, 0.1, l, u, S, S_min, S_plus).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v--Zs9ELLyiB"
   },
   "source": [
    "**Q: What do the dual network outputs tell you about the robustness of the classifier? How does this compare to the classifier's performance (in particular, the $\\vec{c}_j^\\top \\vec{\\hat{z}}_3$ scores) on FGSM outputs? How does your answer change with epsilon?**\n",
    "\n",
    "A: Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dr3xSI7EMv4l"
   },
   "source": [
    "**Q: Suppose you have a deep neural classifier that you want to defend against adversarial attacks. That is, you want to detect and discard any input images that were possibly adversarially perturbed. How might you do this with the robustness certificate you have implemented?**\n",
    "\n",
    "A: Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s--Ae8VpLyiN"
   },
   "source": [
    "## Optional: Robust training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LohSkAUMLBfa"
   },
   "source": [
    "The following function should implement the robust loss from section A of the PDF. This loss is an upper bound on the worst-case loss within an $\\epsilon$ ball of the original training input. Thus, training using this new loss should result in a classifier that is more robust than one trained on the original cross-entropy loss.\n",
    "\n",
    "There are no mandatory questions here, but feel free to experiment with this robust training, and compare the performance here (measured by the dual objective certificate, as well as original/FGSM accuracy) to that of the original. You can also try training a model using the original loss first, then fine-tuning with the robust loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uTv9zjuqLyiQ"
   },
   "outputs": [],
   "source": [
    "def robust_loss(x, label, net, eps, criterion):\n",
    "    '''\n",
    "    Given a batch of input images X, its corresponding lables LABEL,\n",
    "    the classifier NET, epsilon value EPS, and original loss\n",
    "    function CRITERION, returns the robust loss of NET w/r/t\n",
    "    the original loss function, on the input image.\n",
    "    '''\n",
    "    l, u, S, S_min, S_plus = dual_bounds(x, net, eps)\n",
    "    # We assume there are 10 classes.\n",
    "    e_y = torch.zeros(10, 1)\n",
    "    e_y[label] = 1\n",
    "    c = e_y @ torch.ones(1, 10) - torch.eye(10)\n",
    "    J = dual_forward(x, net, c, eps, l, u, S, S_min, S_plus).unsqueeze(0)\n",
    "    return criterion(-J, label.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nj_VkNaNLyiY"
   },
   "outputs": [],
   "source": [
    "def robust_train(net, criterion, trainloader, eps, lr=0.001):\n",
    "    '''\n",
    "    Trains the classifier NET using the robust version\n",
    "    of the original loss function CRITERION with paramater EPS,\n",
    "    using training data from TRAINLOADER and with learning rate LR.\n",
    "    \n",
    "    Note that we half the learning rate each epoch.\n",
    "    '''\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(3):\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr * 0.5 ** (epoch)\n",
    "\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            loss = 0\n",
    "            for i in range(inputs.shape[0]):\n",
    "                x = inputs[i].unsqueeze(0)\n",
    "                label = labels[i].unsqueeze(0)\n",
    "                loss += robust_loss(x, label, net, eps, criterion)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            if i % 500 == 0:\n",
    "                print('Epoch', epoch, 'Iter:', i, 'Loss', loss.item())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "torch_impl_sol.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "080f0e9e20374c8b9d0772c207d40204": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0d2ab5e4e4634c2d84445dcb59e63093": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "0ffb65b8fcd64017a1575ef5c6fb6783": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e0fc65863e247bba684aa570176e47e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9b0d7cc1d4a84b32ad704d0f92aaf7c2",
       "IPY_MODEL_359ca0eaed76461b8920085a62f10151"
      ],
      "layout": "IPY_MODEL_0ffb65b8fcd64017a1575ef5c6fb6783"
     }
    },
    "2dfa9272bd3a454582ebcd023d814301": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5cdd60a9989c4e2fba367aa0fc23db04",
      "placeholder": "​",
      "style": "IPY_MODEL_77a3b32f508c46f4a5bae2a29bbfc4f2",
      "value": " 8192/? [00:06&lt;00:00, 1348.53it/s]"
     }
    },
    "359ca0eaed76461b8920085a62f10151": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_080f0e9e20374c8b9d0772c207d40204",
      "placeholder": "​",
      "style": "IPY_MODEL_7cb630c6d3784c0ba705b39311a36690",
      "value": " 0/28881 [00:00&lt;?, ?it/s]"
     }
    },
    "3ce16927200e4dfbba645b52ede3be15": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_51fab2724f5240f2a197da36bfb5d37f",
      "placeholder": "​",
      "style": "IPY_MODEL_7f3be4825b3942bfbd15404ae2826968",
      "value": " 9920512/? [00:20&lt;00:00, 1533441.80it/s]"
     }
    },
    "40781e910ad14961b70299654d08af57": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "410b4797b4eb474185b6e9e8768efe90": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "443bc19134b346e282b58b81de72ee9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d51c1aaaee7e40eb9a6946ee964f26db",
       "IPY_MODEL_75a9df895ea94bb787ead957f9b540c9"
      ],
      "layout": "IPY_MODEL_be2ab404919947b6bec6e47d8ef2293e"
     }
    },
    "48f34dff73514ae58be4a78b897605de": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "51fab2724f5240f2a197da36bfb5d37f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5cdd60a9989c4e2fba367aa0fc23db04": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "68fb67ba49274e2381586dfa12d4a00a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "73155d071971464cac0ed23138cde635": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "75a9df895ea94bb787ead957f9b540c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ad74039e867d489fbcc6c9d8ff34706c",
      "placeholder": "​",
      "style": "IPY_MODEL_73155d071971464cac0ed23138cde635",
      "value": " 1654784/? [00:06&lt;00:00, 244349.52it/s]"
     }
    },
    "77a3b32f508c46f4a5bae2a29bbfc4f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7cb630c6d3784c0ba705b39311a36690": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7f3be4825b3942bfbd15404ae2826968": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "86cdc4424b24473a8d571d2160ee7576": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8a08a24593a2422985302484dc4594d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "8c6d1f7d352a4c089e4ae601a73c1068": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_410b4797b4eb474185b6e9e8768efe90",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8a08a24593a2422985302484dc4594d1",
      "value": 1
     }
    },
    "9601fc4be35543a784d4231a2de31edb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9b0d7cc1d4a84b32ad704d0f92aaf7c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "  0%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c75be203b59b4c599fd51b44b5359236",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0d2ab5e4e4634c2d84445dcb59e63093",
      "value": 0
     }
    },
    "9d5649b6391e44f5ad266c65fe71ec8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_de997021a80b4943b20c5518910ea387",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_68fb67ba49274e2381586dfa12d4a00a",
      "value": 1
     }
    },
    "ad74039e867d489fbcc6c9d8ff34706c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bde4879694234dc087cc6ef3a8f999e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9d5649b6391e44f5ad266c65fe71ec8e",
       "IPY_MODEL_2dfa9272bd3a454582ebcd023d814301"
      ],
      "layout": "IPY_MODEL_86cdc4424b24473a8d571d2160ee7576"
     }
    },
    "be2ab404919947b6bec6e47d8ef2293e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c33f5dd2c1f443f1b771f0c46054eebf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8c6d1f7d352a4c089e4ae601a73c1068",
       "IPY_MODEL_3ce16927200e4dfbba645b52ede3be15"
      ],
      "layout": "IPY_MODEL_9601fc4be35543a784d4231a2de31edb"
     }
    },
    "c75be203b59b4c599fd51b44b5359236": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d51c1aaaee7e40eb9a6946ee964f26db": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_48f34dff73514ae58be4a78b897605de",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_40781e910ad14961b70299654d08af57",
      "value": 1
     }
    },
    "de997021a80b4943b20c5518910ea387": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
